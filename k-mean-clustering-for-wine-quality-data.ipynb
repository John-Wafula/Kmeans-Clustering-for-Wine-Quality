{"cells":[{"metadata":{"_uuid":"038d9acdcc2f57de05300d5030c5235079499e80"},"cell_type":"markdown","source":"# Clustering - Wine Quality Data\n\nClustering leads to new discovery of knoweldge.\n\nClustering is an branch of ***Unsupervised Learning***. \n\n**Theory for Clustering**\nBasic Req. when we can say we have clusters.\n* There must be some way to say that 1 observation is closer to A observation than B.\n* There must be some proximity measure or similarity measure between data points of dataset.\n* Object should be as homogenous as possible in 1 cluster and object point between 2 cluster should be as homogenous as possible.\n* Way to know that this cluster is good enough and no more clustering is required.\n* Proximity Measure.\n* Goodness of fit function.\n* Clustering must be effective i.e it should be complete and correct.\n\n**Req. for a Good Clustering Algorithm**\n* Scalable (independent of size of data).\n* Should be able to deal with different types of data.\n* Whatever may be the shape of cluster, the algo shoud be able to handle the clustering.\n* A good clustering solution will remove Noise and Outliers.\n* Whatever order the data is feed into algo, the cluster should always be the same.\n* Interperoable\n\nAll above req. are in ideal scenario.\nAlgo we will be using are not be able to fulfill all req."},{"metadata":{"_uuid":"2c27f6a7f4d7857abebe206f817b1449dc05610f"},"cell_type":"markdown","source":"## Various techniques used - "},{"metadata":{"_uuid":"1a555f2919bf241bb156ad08245b40efde81fa6e"},"cell_type":"markdown","source":"We are going to use Wine quality data provided to us, and will try to group them and create clusters based on simialrities between them. \n- We will be making use of K-mean clustering techniques. \n- To make sure the rsults we will make use of Data manipulation and data analysis so that we get good clustering results. \n- We will also make use of various techniques to find homoginity and numbers of clusters we should create to gt best results.\n- Silhouette Algorithm.\n- Hokins Stats.\n- Sum of Squared errors. SSE\nand others"},{"metadata":{"trusted":true,"_uuid":"babf7c2c72402736a7ed549538833aea8d359925"},"cell_type":"code","source":"#Loading Libraries\n# Call libraries and read data\nlibrary(dplyr)          # For data manipulation ,filtering etc\nlibrary(magrittr)       # For pipes\nlibrary(caret)          # For dummy variables, nearZeroVar(), findCorrleation()\nlibrary(ggplot2)        # For plotting\nlibrary(ggthemes)       # For a variety of plot-themes\nlibrary(gridExtra)      # Arranging ggplots in a grid\nlibrary(lattice)\nlibrary(vegan)\nlibrary(NbClust)\nlibrary(cluster)        # For silhoutte()\nlibrary(factoextra)     # get_clust_tendency() assesses hopkins stat\nlibrary(clustertend)    # Another package for hopkins() function\nlibrary(data.table)\nlibrary(GGally)\nlibrary(ggcorrplot)\nlibrary(mclust)\nlibrary(fpc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"051ddb6bf4a60f2fa28b21a8c6eb2aa0b5fa8116"},"cell_type":"code","source":"#Lets clear any presvious stored variables and do garbage collection. This will remove unwanted variables and free up memory.\nrm(list=ls());gc()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d82432d0ab6d58e957a07e48f2232d04fe2c3386"},"cell_type":"markdown","source":"### Reading Data\n\nFor reasding data from csv file we will be making use of **\"read.csv\"**.\nWe can also use **\"fread\"** too but this was giving some issue with aes_string used in ggplot.\n**Reason**: the column names had spaces when reading data using fread where as read.csv replaces spaces with '.'"},{"metadata":{"trusted":true,"_uuid":"96802e073726143b797444c3a7aa3132b2772fe6"},"cell_type":"code","source":"\n#Reading the data from data set available, which will be used for clustering.\nwineData <- read.csv(\"../input/winequality.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7be14dd14ad1ce5bcf4cc322363bbd4f4b31302e"},"cell_type":"markdown","source":"## Understanding Data\nWe will run some queries and do some checks to see how good and complete the data is. \nAny data we use should be checked, this will help in determining what all we need to do to make our analysis and predicition accurate."},{"metadata":{"trusted":true,"_uuid":"5e7829c9a129cda9a554b602bcc838316f0ab754","scrolled":true},"cell_type":"code","source":"options(scipen = 999)\nset.seed(321)\n#Let us start buy looking at the strucutre of data we are going to use.\nstr(wineData)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e9a957cb7caf1ca47ccfdd83a504232fb129807"},"cell_type":"markdown","source":"***Note:*** Let us note here the class types of all columns. Will clarify in subsequest steps why it is important.\nAlso you can see how we have '.' in column names, that us because we have used \"read.csv\""},{"metadata":{"trusted":true,"_uuid":"295e5a30342bab0e85da56c171464079461c0b2f"},"cell_type":"code","source":"#Some more details related to data.\n#How data looks\nprint(\"---Data preview---\")\nhead(wineData)\n\n#Number of rows to identify how big is the data we are dealing with\nprint(\"---Number of rows---\")\nnrow(wineData)\n\nprint(\"---Column names---\")\n#Names of all columns\nnames(wineData)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3ac9a3919e49aee6b5f63c759a5dfeecfb93946"},"cell_type":"code","source":"#Let us check if we have any NA values in our data.\n#We should remove and NA or incomplete data.\n#If FALSE means no NA data in our data.frame\n#If TRUE we will check each column and for NA data.\n\nprint(\"----Checking for NA Data---\")\nany(is.na.data.frame(wineData))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08f6d9b7029fbeab915a1801567332589becf0dd"},"cell_type":"code","source":"#Checking values in Columns \"Color\", \"Quality\" and \"Good\".\n#Might be helpful in subsequest iterations.\n\nprint(\"---Values in color column\")\nunique(wineData$color)\n\nprint(\"---Values in good column\")\nunique(wineData$good)\n\nprint(\"---Values in quality column\")\nunique(wineData$quality)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b19abdee3d1e9ba03f6412b912a374ebc6cce048"},"cell_type":"markdown","source":"***Note-*** We can make use of above information to do some factor related computations. This can also be helpful in plotting graphs."},{"metadata":{"_uuid":"f2c70f4bdc3d7564411196c9238f432d297d1dcd"},"cell_type":"markdown","source":"Based on the above values, let us use some graphs and see frequency distribution ( *histogram* ) of **Red** and **White** wines across different quality."},{"metadata":{"trusted":true,"_uuid":"4ce0e42b71b30a3ffc3cf705fee066a47189c791"},"cell_type":"code","source":"ggplot(wineData, aes(quality,fill=color, color=c(\"red\", \"white\"))) +\n    geom_histogram(binwidth = .5,col=\"black\") +  \n        facet_grid(color ~ .)+\n        labs(title=\"Histogram Showing Qulity of Wine\", \n        subtitle=\"Wine Quality across Red and White colors of Wine\") ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fc3de13f526d19f46d5b711d1bd942a25bd7823"},"cell_type":"markdown","source":"Let us plot graphs for all other columns, to see how the points are distributed based on different qualities.\nWe will make use of scatter plot.\nWe also show the distribution for Red and White Wines.\n\n(To make our life easier we will be using for loops, this will help in removing duplicate code.)"},{"metadata":{"trusted":true,"_uuid":"cf1b55874a1f7e7583093111bb82f934e81d57f4"},"cell_type":"code","source":"for(i in 1:11){\n    print(paste(\"---Plot for---\", colnames(wineData)[i]))\n\n#Overall distribution\n  print(ggplot(wineData, aes_string(\"quality\", colnames(wineData)[i]))+\n        geom_count(col=\"tomato3\", show.legend=F)+\n        theme_solarized())\n\n#Color wise scatter plot \n  print(ggplot(wineData, aes_string(\"quality\", colnames(wineData)[i]))+\n        geom_jitter(aes(col=as.factor(color))))\n}\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ec5d03525cce2e5c5eefeb5f4e209a958972394"},"cell_type":"markdown","source":"***Note-*** We are making use of *print()* function to plot graphs, this is because when using in ggplot in loops it will not print graphs until and unless called with in *print()* function."},{"metadata":{"_uuid":"a43cbe41c317f0cab5baf4f96790db8a762dc995"},"cell_type":"markdown","source":"By looking at graphs we can easily tell content of wines based on quality.\nAlso we can tell how content changes across **Red** and **White** wine."},{"metadata":{"_uuid":"8c9cca2b2a89d04ef6fd1f4dde9e86cf98591f90"},"cell_type":"markdown","source":"# Start of Clustering\nWe have gone through the data available in hand and ran tests which has given us some clarity over data, and we can start our clustering analysis.\n\nFor successfull clustering and clustering to be good we need to follow few steps, before we can do actual clustering.\n\n## Objectives: \n###        a. Understanding data\n###        b. Data pre-processing\n####               i)   Creating dummy variables\n####               ii)  Removing columns with zero-variance\n####               iii) Scaling and centering data\n####               iv) Discover and remove higly correlated variables\n####               v)  Data Transformation. Scaling and Centering\n###         c. Determining Number of Clusters.\n###         d. Segmentation using k-means\n###         e. Clustering Tendency of data\n\n***Note*** Steps involved in Data pre-process depends on the data we are dealing with. Thats why understanding data is very important. \n\nWe have already performed **Step: a. Understanding Data**.\nNow let us make use of same and perform our analysis."},{"metadata":{"_uuid":"7fddd2ae48267b2b8e10a5999e9be778b4311f75"},"cell_type":"markdown","source":"## b. Data PreProcessing.\n\nData pre-processing involves various process. We will define the process required as we proceed. All these steps will help in achiving good clustering.\nWill also tell steps which we can do but might not required here depending on the data."},{"metadata":{"_uuid":"2a3697b90286c9ce1daa94989148924fcad71c65"},"cell_type":"markdown","source":"### Step 1 - Changng all column values to Numeric\n\nRemeber, previously i asked to make a note of class of columns. This where we will make use of it. \nGoal here is to convert all and any columns to numerical value.\n\n*Clustering can be performed only for numerical values.*\n\nBased on **str()** called earlier we have seen that all our columns are of class Num accept - *Quality, Good* (of int type) and *Color* (of factor type).\n\nWe will not make changes to Color column as we will use it as is for plotting graphs.\nAnd just to be carefulll we will convert int -> numeric."},{"metadata":{"trusted":true,"_uuid":"7f53385229070c942d446d8a44e756974d189866"},"cell_type":"code","source":"#Making changes for converting data types\n\n#Converting 2 interger columns too numeric\nwineData$quality <- as.numeric(wineData$quality)\nwineData$good <- as.numeric(wineData$good)\n\n#for ease creating a data set without color column\nwineData_no_color <- wineData[1:13]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"629492ff14b7b8e166be7e5c94fd3ee3a0bf7f9c"},"cell_type":"code","source":"#Let us see the structure again\nstr(wineData_no_color)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc920c6f0e6a68c978b2243f3461d7e35cedc322"},"cell_type":"markdown","source":"All columns numeric accept color, the way we want it.\n\n***FYI***\n*Just to add how we can convert factors to numeric\nReplace levels of factors with numbers*\n*levels(wineData$color) <- c(0,1)*\n\n*Now Converting them too Numerical value*\n*wineData$color<- as.numeric(wineData$color)*\n\nWe have successfully converted factors to numerics"},{"metadata":{"_uuid":"2ab691ace385665b735b29e30d3f56a8046b74bc"},"cell_type":"markdown","source":"### Step - 2 Try finding Near Zero Variance\n\nIn this step we will try to find out columns which might not have any significant variations in there values, and there variance doesn't make any difference to results.\n\nWe can remove those coluns and make our computation faster.\n\n*We will make use a function from **Caret** library for this.* "},{"metadata":{"trusted":true,"_uuid":"e864d9755a3f5b0aa23cbf0b7e38aa632d8638f6"},"cell_type":"code","source":"#Will give us column number which might insignificant variance.\nnzv <- nearZeroVar(wineData)\n\nprint(paste(\"---Column number with----\", nzv))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f8eb7105483f4db04510ea14bf12f887bee296e"},"cell_type":"markdown","source":"As we can see nothing is returned i.e we don't have any such column.\n"},{"metadata":{"_uuid":"9175e7fdfbfb48a73c4ffaa83469d50e1880017b"},"cell_type":"markdown","source":"### Step 3 - Normalizing the Data i.e Scaling and Centering\n\nIn this we step we try to normalize the values of column so that any columns with large values compared to others may not dominate columns with lower values.\nThis columns with higher values may cause inconsistency in clustering.\n\nLet us identifying columns with large data.\n\nThere are 2 ways of doing it but we will make use of a technique which will give us all values in the range of 0's and 1's"},{"metadata":{"trusted":true,"_uuid":"6eeb9b4c3dc21245a283cbc8c627ad2012476514"},"cell_type":"code","source":"#Identifying columns with large data\nhead(wineData)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"823231fcb8f9fc152bdf7e5bda74925c3482231d"},"cell_type":"code","source":"#By looking at data we can say that Columns 1, 4, 6, 7, 9, 11, 12 can cause issues, we will try to normalize them so that there value are in 0 to 1 range.\nprint(\"---Normalizing Data----\")\nnorm_data <- sapply(wineData[,c(1,4,6,7,9,11,12)], function(x) (x - min(x))/(max(x) - min(x)))\n\nprint(\"---Type of returned data\")\nclass(norm_data)\n                    \nprint(\"---Converting data from matrix to data.frame---\")\nnorm_data <- data.frame(norm_data)    # norm_data is a 'matrix'\n\nprint(\"---Normalised data---\")\nhead(norm_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c916cd5736ce164cfbfb2d45682293ca3522a7c"},"cell_type":"markdown","source":"Brief ***theory*** on what we have done here.\n\n*(Each Data point in column subtracted by minimum value if that column) divided by (maximum value of that columns subtracted by minimum value of that column).*\n\nThe result of this will always give a value which will be in range of 0 to 1"},{"metadata":{"_uuid":"5779401dab9b95999f1e58b43bc9cbd939e64ed7"},"cell_type":"markdown","source":"Now let us bind the rest of data with normalized data and see the data.\n"},{"metadata":{"trusted":true,"_uuid":"f93f05b1fb88428513c3f8c79a9027c04ceb8dac"},"cell_type":"code","source":"#Binding the normalised data with other data\nwineData_norm <- cbind(wineData[,c(2,3,5,8,10,13)],norm_data)\nhead(wineData_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92dde6efb39938de61fe5ab3fcd727667cee98d0"},"cell_type":"code","source":"str(wineData_norm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d231b8d244a43c650bdcde8da946b7fcee6e795"},"cell_type":"markdown","source":"Alternatively we can make use of ***scale()*** method for normalisation.\nLet us see how the data looks with scaling and will do comparison for data."},{"metadata":{"trusted":true,"_uuid":"ce834f82d62eeb402b318a0ffc44e81b89571f5d"},"cell_type":"code","source":"wineData_scaled <- scale(wineData_no_color)\n\nhead(wineData_scaled)\n\nclass(wineData_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f34d47806207cf3703637650afc84d27ff20829"},"cell_type":"code","source":"#Converting to data.frame\nwineData_scaled_df <- as.data.frame(wineData_scaled)\nclass(wineData_scaled_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83c46a023111645b72e47d7412bd09fc2d2ff6ae"},"cell_type":"markdown","source":"### Step 4 - Finding higly corelated columns.\n\nIn this step we will try to determine columns which are highly correlated to each other.\nThis tells if the values of 1 column are changing then the value in other column will also change in same manner. \n\nFor Ex: Income and Taxes.\nAs Income will go up taxes will also go up and vice-versa.\n\nBy this we can easily say that we can remove 1 of the two corelated columns and it will not effect our clustering results."},{"metadata":{"_uuid":"b2799e7df920797fbabc4f45c4c06b6b6031104e"},"cell_type":"markdown","source":"To Find correlation we will make use of both Visuale as well as data based techniques."},{"metadata":{"trusted":true,"_uuid":"c3574fc302a06ae7b94ade40acab6ee51b2d2ee5"},"cell_type":"code","source":"#Lets find correlation using cor()\ncorr_norm <- round(cor(wineData_norm),1)\ncorr_norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"508da20e40402a5a74e2d148168fc5a6a3a7c055"},"cell_type":"code","source":"#Let us plot a Correlogram using above returned results.\nggcorrplot(corr_norm, hc.order = TRUE, \n           type = \"lower\", \n           lab = TRUE, \n           lab_size = 3, \n           method=\"circle\", \n           colors = c(\"tomato2\", \"white\", \"springgreen3\"), \n           title=\"Correlogram of Wine Data\", \n           ggtheme=theme_dark)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58f3411cefc3735ec478a7a84ad5df25214d1f76"},"cell_type":"markdown","source":"By looking at above graph we can tell how each correlated with other columns.\nAnd we can *visually analyze* that column ***'Quality'*** and ***'Good'*** has the ***maximum*** correlation of ***0.8***.\n\nIt is upto us and requirment to determine at what level of correlation we want to consider for removing from cluster equation.\n\n*For us we are considering level 0.7 and anything above it can be removed.*\n\n(For data interpreation we will again make use of a fuction available in *Caret* library.)"},{"metadata":{"trusted":true,"_uuid":"b22a34d7abf541cc2fc212a2734c36d4fb20c20b"},"cell_type":"code","source":"corr_scaled <- round(cor(wineData_scaled_df),1)\n\n##Lets plot same for Scaled data\nggcorrplot(corr_scaled, hc.order = TRUE, \n           type = \"lower\", \n           lab = TRUE, \n           lab_size = 3, \n           method=\"circle\", \n           colors = c(\"tomato2\", \"white\", \"springgreen3\"), \n           title=\"Correlogram of Wine Data\", \n           ggtheme=theme_dark)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"617284d0dd771d035c023b3685fe0b36e412beae"},"cell_type":"markdown","source":"***Note:*** *Same results i.e we can use any one of them. We will make use normalised data*"},{"metadata":{"trusted":true,"_uuid":"4be0b7b8480ecd91261d688e72a40e4083f1ba5b"},"cell_type":"code","source":"#Let us calculate it thought data.\n#Again we will make use of Caret Library.\na<-findCorrelation(corr_norm, cutoff = 0.7, verbose = T)\n#Returns an integer value\n\nprint(\"--- Columns number---\")\na\n\nprint(\"---Column name we want to remove---\")\ncolnames(wineData_norm)[a]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15a214633e139fb02de2255dac748aa2d0248acb"},"cell_type":"code","source":"#Let us remove column \"Good\" from our clustering equation.\n#Removing Good column as high quality wine will be good\nwineData_scaled_df$good <- NULL\nstr(wineData_scaled_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db05e8c538c5a5ce1b3a253cf74d642b22143644"},"cell_type":"markdown","source":"### End of Data Pre-Processing step."},{"metadata":{"_uuid":"cd3f0c5469500e1d74dcab08eea8131e4c418dcb"},"cell_type":"markdown","source":"## c. Determining Number of clusters.\n\nK-Mean makes us of number of clusters as one of the inputs.\n\nThe numbers of clusters which be best for us varies from data to data.\nThere are various techniques which can be used to determine the number of clusters best suited for the data.\n\nWill try to make use of few. We will also include graphs represnations for visual analystics."},{"metadata":{"_uuid":"cf8c50a2dca9ac774cc195cd7bd046d8390cea2e"},"cell_type":"markdown","source":"#### Method: 1 - Elbow method\n\nElbow method makes of use ***Sum of Squared Error*** or ***Within-cluster sum of square (WSS)***.\n\nIt is nothing but distance of all points in a cluster from its center, square it and sum them up for all points.\n\nVisually it is the point in graph from were we starting no change or a straight line. i.e increase in number of clusters is practically making no difference to our data points.\n\nThis technique makes use calling k-means for a range of cluster i.e from 1 to certain number and gives us the clustering data.\nThen we can plot the same data to see the behaviour.\n\n*Ref: http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/ *"},{"metadata":{"trusted":true,"_uuid":"287bf2051ee755dfdda686a1ba0fadf9b933913a"},"cell_type":"code","source":"# Initialize total within sum of squares error: wss\nwss <- 0\n\n# For 1 to 20 cluster centers\nfor (i in 1:10) {\n  km.out <- kmeans(wineData_norm, centers = i)\n  # Save total within sum of squares to wss variable\n  wss[i] <- km.out$tot.withinss\n}\nwss\n# Plot total within sum of squares vs. number of clusters\nplot(1:10, wss, type = \"b\", \n     xlab = \"Number of Clusters\", \n     ylab = \"Within groups sum of squares\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7824a70cc47383ebd523f00474a390f0ab0d7715"},"cell_type":"markdown","source":"#### Method: 2 Silhouette plot\n\nIn this we will make used NbClust, after trying multiple time it was found that the large data set can some time cause issues in getting out puts with NbClust.\nTo tackle this issue we will make use sample data and will do our analysis over it."},{"metadata":{"_uuid":"36c743bb3cfe8a049794050290977625c1747a78"},"cell_type":"markdown","source":"***Sampling Data***\nWas not able to find many methods, but few are\n**sample(),\nsample_frac() - dplyr,\ncreateDataPartition() - caret**\n\nWe will make use of sample_frac()"},{"metadata":{"trusted":true,"_uuid":"441d38d7ccce7a87e8047e48cfa7f2bf211d920f"},"cell_type":"code","source":"wine_train_data <- sample_frac(wineData_norm, 0.65)\n\nhead(wine_train_data)\n\nstr(wine_train_data)\n\nnrow(wine_train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32fcce22ef509f0bdca654e449bfc895d31166ca"},"cell_type":"code","source":"#Let us plot Silhouette Plot using NbCluster\nfviz_nbclust(wine_train_data, kmeans, method = \"silhouette\")+\n  labs(subtitle = \"Silhouette method\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca987781151633596a89e533cc97ac2346baa5fb"},"cell_type":"markdown","source":"There are more methods to find out cluster numbers.\nYou can check out the ref link."},{"metadata":{"_uuid":"e1781658f941ce15e70613ef0d080adf82888628"},"cell_type":"markdown","source":"## d. Segmentation Using K-mean\n\nLet us do the clustering using the numbers of cluster we have got from above analysis and see our outputs.\n\nWe are making use od Cluster - 2 for our analysis."},{"metadata":{"trusted":true,"_uuid":"955afcd7b7451120168a3268e211fc1e592c8873"},"cell_type":"code","source":"km <- kmeans(wineData_norm, 2, iter.max = 140 , algorithm=\"Lloyd\", nstart=100)\nkm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cdb57a06584d47952d20c6712a59c4ade7bff96"},"cell_type":"code","source":"#Structure of km\nstr(km)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef867288f5ae83581bc26874c5a5eb0857e77e5d"},"cell_type":"markdown","source":"Let us do visual analysis of data using results from K-mean. We will try to plot different graphs for better understanding."},{"metadata":{"trusted":true,"_uuid":"fddaf3c9c9984c5a1f11ec2e36a44f3d6bf390a8"},"cell_type":"code","source":"# Centroid Plot against 1st 2 discriminant functions\nclusplot(wineData, km$cluster, color=TRUE, shade=TRUE, \n         labels=2, lines=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71263b81d0010bd7b24868148fd2ded24115f95d"},"cell_type":"code","source":"fviz_cluster(km, data = wineData_norm,\n             ellipse.type = \"convex\",\n             palette = \"jco\",\n             ggtheme = theme_minimal())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f99ecf63fc03f93d877b37c574435466fdb0971"},"cell_type":"code","source":"fviz_cluster(list(data = wineData_norm, cluster = km$cluster),\n             ellipse.type = \"norm\", geom = \"point\", stand = FALSE,\n             palette = \"jco\", ggtheme = theme_classic())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fac6c0702632df3826a91cd386fb3bc3e6ae91d1"},"cell_type":"code","source":"pam.res <- pam(wineData_norm, 2)\n# Visualize\nfviz_cluster(pam.res)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbed8d20d04651fd39bbb91a6a0b7143dda3eef5"},"cell_type":"markdown","source":"## f. Clustering tendency\n\nNow let us try to determine if our data has a tendency to be clustered or we are forcing it into clusters.\nTo check tendency of clustering of our data we will make use of a randome data set, and will compare the both the results to see the results.\nWe will make use Visual analysis for this."},{"metadata":{"trusted":true,"_uuid":"16790fd67aa19dd3ee5273312bef391d760440a1"},"cell_type":"code","source":"# Random data generated from the iris data set\nrandom_df <- apply(wineData_norm, 2, \n                function(x){runif(length(x), min(x), (max(x)))})\nrandom_df <- as.data.frame(random_df)\n\nhead(random_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"262667fea2c4eefb84b26cbb14eddc756211bb12"},"cell_type":"code","source":"# Plot faithful data set\nfviz_pca_ind(prcomp(wineData_scaled_df), title = \"PCA - Wine data\", \n             habillage = wineData$color,  palette = \"jco\",\n             geom = \"point\", ggtheme = theme_solarized(),\n             legend = \"bottom\")\n# Plot the random df\nfviz_pca_ind(prcomp(random_df), title = \"PCA - Random data\", \n             geom = \"point\", ggtheme = theme_solarized())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61480deef3985a81c02f425c273657c927395e01"},"cell_type":"markdown","source":"Here, we present two R functions / packages to statistically evaluate clustering tendency by computing the Hopkins statistics:\n\n**get_clust_tendency()** function [in factoextra package]. It returns the Hopkins statistics as defined in the formula above. The result is a list containing two elements:\n1. hopkins_stat\n2. plot\n\n**hopkins()** function [in clustertend package]. It implements 1- the definition of H provided here.\n\n*library factoextra*"},{"metadata":{"trusted":true,"_uuid":"48ae53b8c9634898fa96eda803e16ae05c40a38b"},"cell_type":"code","source":"# Compute Hopkins statistic for Wine dataset\nres <- get_clust_tendency(head(wine_train_data, 500), n = 499, graph = TRUE)\nres$hopkins_stat\nplot(res$plot)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e712cb42848dd1666b1c08660af11f3ae9a5f4bf"},"cell_type":"markdown","source":"Hopkins statistic should be as small as possible (near to 0), for doing clustering."},{"metadata":{"trusted":true,"_uuid":"3bfa4b518af709f493d798aa59b203852955e0e3"},"cell_type":"code","source":"# Compute Hopkins statistic for random dataset\nres <- get_clust_tendency(head(random_df, 500), n = 499, graph = TRUE)\nres$hopkins_stat\nplot(res$plot)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8ca5528cb4b652105b1b3bf404df973474b894a"},"cell_type":"markdown","source":"As you can see the random data has a higher Hopkins value.\n\n***Note: *** Hopkins was taking a lot of time thats why had to use a smaller value dataset."},{"metadata":{"_uuid":"028166e1b01c3d6da707ac2d14e04a6722099d7d"},"cell_type":"markdown","source":"#### Various issues faced and there solutions that might be helpful for others.\n1. Working with nbclust was very difficult, ran into a lot of issue due to high computations.\n    Will suggest to take training data set of some size and work on that and then gradually increase the size of training data.\n    \n2. Sampling dataset techniques.\n    I have included techniques which I was able to find out. Provide your comments if you have any more suggestions on sampling data into random formats.\n    \n3. Not all clustering techniques will give same result as they follow different approaches to calcualte clusters.\n"},{"metadata":{"_uuid":"04790d043cc4c1010cbc3a721e17d8e9a8705a59"},"cell_type":"markdown","source":"### Thank you for checking the kernel. If you liked it then do vote-up and provide your valuable comments.\nI will try to incoporate more Clustering techniques to see various results in coming future.\nAny particular topic you will like me to cover or I might have missed please let me know in the comments."},{"metadata":{"trusted":false,"_uuid":"214a026173adc389e508ddf6f48c3cbae6178aff"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}